\chapter{Embedding}
\section{Airbnb Embedding}
Airbnb 将 embedding 技术分别用在召回和排序模型\citerb{2018-Airbnb-embedding}。

\subsection{Listing embedding}
\paragraph{模型输入输出} 输入为若干个用户点击序列 $s=(l_1,\ldots,l_M) \in \mathcal{S}$，输出为每个
listing 的 $d$ 维 embedding 向量，使相似 listing 的向量的距离尽可能小。

\paragraph{损失函数}
\begin{equation}
  \mathcal{L} = \sum_{s \in \mathcal{S}} \sum_{l_i \in s} \sum_{-m \leq j \leq m, i \neq 0} \mathrm{log} \mathrm{P}(l_{i+j}|l_i)
\end{equation}

其中
\begin{equation}
  \mathrm{P}(l_{i+j}|l_i) = \frac{\mathrm{exp} v_{l_i}^{\mathrm{T}} v_{l'_{i+j}}}{\sum_{l=1}^{|V|}\mathrm{exp}(v_{l_i}^{\mathrm{T}} v'_l)}
\end{equation}

\paragraph{训练} 由于 listings 数量较大，无法同时对所有样本进行训练，因此采用负
采样的方法，即随机选择一些不在该 listing 附近的样本作为负样本，将损失函数改为如
下形式：

\begin{equation}
  \mathrm{argmax}_{\theta} \sum_{(l, c) \in \mathcal{D}_{p}} \mathrm{log} \frac{1}{1 + e^{-v'_c v_l}} + \sum_{(l, c) \in \mathcal{D}_{n}} \mathrm{log} \frac{1}{1 + e^{v'_c v_l}}
\end{equation}

\begin{itemize}
  \item 改进 1：加入 booking listing 作为 global context。正样本中加入最终的
    booking listing。
    \begin{equation}
      \mathrm{argmax}_{\theta} \sum_{(l, c) \in \mathcal{D}_{p}} \mathrm{log} \frac{1}{1 + e^{-v'_c v_l}} + \sum_{(l, c) \in \mathcal{D}_{n}} \mathrm{log} \frac{1}{1 + e^{v'_c v_l}} + {\color{red}{\mathrm{log} \frac{1}{1 + e^{-v'_{l_b}v_l}}}}
    \end{equation}
  \item 改进 2：加入与 central listing 属于同一个地区的针对性负样本。
    \begin{equation}
    \begin{split}
      \mathrm{argmax}_{\theta} \sum_{(l, c) \in \mathcal{D}_{p}} \mathrm{log} \frac{1}{1 + e^{-v'_c v_l}} + \sum_{(l, c) \in \mathcal{D}_{n}} \mathrm{log} \frac{1}{1 + e^{v'_c v_l}} + \\ {\mathrm{log} \frac{1}{1 + e^{-v'_{l_b}v_l}}} + {\color{red}{\sum_{(l, m_n) \in \mathcal{D}_{m_n}} \mathrm{log} \frac{1}{1 + e^{v'_{m_n} v_l}}}}
    \end{split}
  \end{equation}
\end{itemize} 

\subsection{Listing type and user type embedding}
Listing embedding 主要关注用户短期兴趣，为了考虑用户长期兴趣，需要加入 user
embedding。由于 booking 信息的稀疏性，不再学习 listing id 和 user
id 的 embedding，而是改为学习 listing type 和 user type 的 embedding。

\paragraph{模型输入输出} 输入为若干同一个用户的预订序
列 $s=(u_{type_1}l_{type_1}, \ldots, u_{type_M}l_{type_M}) \, \in \mathcal{S}$，输出为
每个 listing type 和 user type 的 embedding。序列中的 user type 和 listing type
根据预订时的情况计算，同一个 user 或 listing 在不同时间对应的 type 可能不同。

\paragraph{损失函数} 与 listing embedding 的损失基本相同，区别是需要分别更新
listing type embedding 和 user type embedding，同时不需要对同一地区的其他
listing 做负采样。考虑到房主的偏好，将拒绝入住的负样本也加入损失函数。

\subsection{排序模型}
Embedding 结合用户短期兴趣和长期兴趣可以召回相似的房源，所有召回房源的排序通过
GBDT 模型实现。

\paragraph{模型输入输出} 输入为用户搜索序列 $D_s = (x_i, y_i), i=1 \ldots K$，其
中 $y_i$ 由 listing 最终的预订、点击、拒绝等状态确定。$x_i$ 包括 listing feature，
user feature，query feature 和 cross feature，以及 embedding feature。

\paragraph{Embedding 计算 feature} 以用户点击的 listing 为例。先按照 market 将
listing 分组，计算出不同 market 的平均 listing，再计算当前 listing 与所有 market
平均 listing 的最大相似度(余弦距离)。

\section{Taobao Embedding}
阿里利用 graph embedding 技术生成淘宝的所有物品的 embedding，并以此作为召回的依
据。

\paragraph{构造物品关系图}
利用用户行为构造物品关系图，物品作为节点，将同一用户连续浏览两个物品的行为转化为
两个物品之间的有向边的权重加 1。实际使用时将刷单用户和误点击的数据清除，同时去掉
商家频繁更新的物品，因为这些商品有可能变为完全不同的商品。

%%% Local Variables:
%%% TeX-master: "../master"
%%% End:
